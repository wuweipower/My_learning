JWT为什么可以防止篡改

Header：头部包括令牌的类型(即JWT)及使用的哈希算法(如HMAC SHA256或RSA) 

Payload：内容也是一个json对象，它是存放有效信息的地方，它可以存放jwt提供的信息字段，比如：iss(签发者),exp(过期时间戳),  sub(面向的用户)等，也可自定义字段。此部分不建议存放敏感信息，因为此部分可以解码还原原始内容。

这两个用BASE64Url编码

Signature：这个部分使用base64url将前两部分进行编码，编码后使用点(.)连接组成字符串，最后使用header中声明的签名算法进行签名。

用到三个参数

```
 HMACSHA256(
    base64UrlEncode(header) + "." +
    base64UrlEncode(payload),
    secret)
```

**第三部分使用签名算法对第一部分和第二部分的内容进行签名，常用的签名算法是 HS256，常见的还有md5,sha  等，签名算法需要使用密钥进行签名，密钥不对外公开，并且签名是不可逆的，如果第三方更改了内容那么服务器验证签名就会失败，要想保证验证签名正确必须保证内容、密钥与签名前一致。**



HTTP本质上就是一个TCP连接，只不过协议规定了使用80端口，以及发送命令或数据的格式，而TCP本身是没有加密的功能。致命的是，HTTP在数据传输过程中，数据就是以明文的方式传输的，由于数据没有被加密，所以很容易出现数据窃听、篡改或者是身份伪造的不安全的行为。

HTTPS是基于HTTP的上层添加了一个叫做TLS的安全层，对数据的加密等操作都是在这个安全层中进行处理的，其底层还是应用的HTTP。HTTPS通信先是使用非对称加密进行密钥的协商，协商出一个对称加密的密钥，之后的通信则采用这个对称密钥进行对称加密密文传输。因为非对称加密其算法极其复杂，导致解密效率低下，而对称加密效率则明显高出百倍。

- 客户端发起`client hello`握手请求,生成一个随机数(简称`CR1`),客户端支持的加密方式等传给服务端
- 服务端收到请求,发起`server hello`,生成一个随机数(简称`SR1`),并且告诉客户端选择的加密方式,并且把服务器证书一起传给客户端.
- 客户端收到请求后,先用证书对应的CA厂商的公钥对证书进行解密,解密后获取到服务器的公钥.然后生成一个随机数`pre-master key`,然后根据`CR1`,`SR1`和`pre-master key`三个随机数,生成一个随机的密钥,这个密钥叫做会话密钥.然后客户端会使用服务器的公钥,对`pre-master key`进行加密,传送至服务端,并且告知服务端之后会使用会话密钥来进行通信(不会直接把会话密钥传过去)
- 服务端收到请求后,通过自己的私钥解密拿到`pre-master key`,然后同样根据`CR1`,`SR1`,`pre-master key`生成会话密钥.此时客户端和服务端通过同样的参数和算法生成了同样的会话密钥,是之后用来对称性加密的密钥.之后向客户端发送最后的信息:随后使用会话密钥通信,结束握手.

1. 如果公钥可信,那么用服务端的公钥能解密,说明服务端的身份是对的.不会有中间人假装是服务端来欺骗你
2. 如果摘要签名是对的,那么说明数据未经过篡改.中间人不会修改信息来欺骗你.
3. 如果CA是可信的,那么CA信任的服务端的公钥则是可信的.



Redis持久化

RDB方式Redis DataBase，是将redis某一时刻的数据持久化到磁盘中，是一种快照式的持久化方法。redis会单独创建（fork）一个子进程来进行持久化，而主进程是不会进行任何IO操作的，这样就确保了redis极高的性能。

AOF，英文是Append Only File，即只允许追加不允许改写的文件。前面介绍的，AOF方式是将执行过的写指令记录下来，在数据恢复时按照从前到后的顺序再将指令都执行一遍，就这么简单。默认的AOF持久化策略是每秒钟fsync一次（fsync是指把缓存中的写指令记录到磁盘中），因为在这种情况下，redis仍然可以保持很好的处理性能，即使redis故障，也只会丢失最近1秒钟的数据。



红黑树与平衡二叉树

红黑树

1.节点是红色或者黑色；

2.根节点和叶子节点是黑色，叶子节点为空节点；

3.每个红色节点的叶子节点都是黑色；

4.从任何节点到叶子节点的所有路径包含相同数目的黑色节点；

5.红黑树实现平衡和保持红黑特征的手段是：变色，左旋和右旋。

平衡二叉树

每个节点的左右子树高度差不超过1。频繁的插入和删除时需要多次旋转来维持平衡，牺牲大量时间。



1.红黑树追求大致平衡，在和平衡二叉树查找时间复杂度相差不大的情况下，保证每次插入最多只用三次旋转就能达到平衡。

2.平衡二叉树追求绝对平衡，插入新节点时旋转的次数不能确定。



一、corePoolSize 线程池核心线程大小

线程池中会维护一个最小的线程数量，即使这些线程处理空闲状态，他们也不会被销毁，除非设置了allowCoreThreadTimeOut。这里的最小线程数量即是corePoolSize。任务提交到线程池后，首先会检查当前线程数是否达到了corePoolSize，如果没有达到的话，则会创建一个新线程来处理这个任务。

二、maximumPoolSize 线程池最大线程数量

当前线程数达到corePoolSize后，如果继续有任务被提交到线程池，会将任务缓存到工作队列（后面会介绍）中。如果队列也已满，则会去创建一个新线程来出来这个处理。线程池不会无限制的去创建新线程，它会有一个最大线程数量的限制，这个数量即由maximunPoolSize指定。

三、keepAliveTime 空闲线程存活时间

一个线程如果处于空闲状态，并且当前的线程数量大于corePoolSize，那么在指定时间后，这个空闲线程会被销毁，这里的指定时间由keepAliveTime来设定

四、unit 空闲线程存活时间单位

keepAliveTime的计量单位

五、workQueue 工作队列

新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。jdk中提供了四种工作队列：

①ArrayBlockingQueue

基于数组的有界阻塞队列，按FIFO排序。新任务进来后，会放到该队列的队尾，有界的数组可以防止资源耗尽问题。当线程池中线程数量达到corePoolSize后，再有新任务进来，则会将任务放入该队列的队尾，等待被调度。如果队列已经是满的，则创建一个新线程，如果线程数量已经达到maxPoolSize，则会执行拒绝策略。

②LinkedBlockingQuene

基于链表的无界阻塞队列（其实最大容量为Interger.MAX），按照FIFO排序。由于该队列的近似无界性，当线程池中线程数量达到corePoolSize后，再有新任务进来，会一直存入该队列，而基本不会去创建新线程直到maxPoolSize（很难达到Interger.MAX这个数），因此使用该工作队列时，参数maxPoolSize其实是不起作用的。

③SynchronousQuene

一个不缓存任务的阻塞队列，生产者放入一个任务必须等到消费者取出这个任务。也就是说新任务进来时，不会缓存，而是直接被调度执行该任务，如果没有可用线程，则创建新线程，如果线程数量达到maxPoolSize，则执行拒绝策略。

④PriorityBlockingQueue

具有优先级的无界阻塞队列，优先级通过参数Comparator实现。

六、threadFactory 线程工厂

创建一个新线程时使用的工厂，可以用来设定线程名、是否为daemon线程等等




多线程的好处

更好的CPU使用率

程序设计更简单

更快的程序响应速度

更公平地分配CPU资源

发挥多核CPU 的优势



线程池的优点

降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。

提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。

提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用[线程池](https://so.csdn.net/so/search?q=线程池&spm=1001.2101.3001.7020)可以进行统一的分配，调优和监控。



线程过多

上下文切换影响性能

线程安全：原子性 可见性

饥饿问题



tomcat线程多，可以改配置，增加最大线程数，



数据库范式

所谓第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值

第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分，这个惟一属性列被称为主关键字或主键、主码。 第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。非主属性非部分依赖于主关键字。

满足第三范式（3NF）必须先满足第二范式（2NF），简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息

二三范式解决

数据冗余、更新异常、插入异常和删除异常。 

**在第三范式的基础上，数据库表中如果****不存在****任何字段****对****任一候选关键字段****的传递函数依赖****则符合第三范式。**



jvm的垃圾回收

jvm元空间大小对于64位JVM来说，元空间的默认初始大小是20.75MB，默认的元空间的最大值是无限。

```
-XX:MetaspaceSize=N`和 `-XX:MaxMetaspaceSize=N
```

如果把-Xss或者-XX:ThreadStackSize设为0，就是使用“系统默认值”。 而在Linux x64上HotSpot VM给Java栈定义的“系统默认”大小也是**1MB**。

要更改堆栈大小，可以使用-Xss调整标志。

-XX:ThreadStackSize



GC Roots：在上面的gc过程中，我们还提到了JVM是如何判断垃圾对象的。简单地来说，就是从gc  roots的根出发(即局部变量表中的引用对象)，一路沿着引用关系找，凡是能够被找到的对象都是非垃圾对象，并且会被移动到下一个它应该去的区域中。剩下的对象，会在区域清空时，一同被清理掉而无须关心。



查看GC情况的命令jstat



jvm调优：吞吐量和响应时间

在1.8以后选择G1，在1.8之前选择ParNew+CMS组合；

规划内存需求

根据实际情况设置升级年龄，最大年龄为15；

设定日志参数 ：GC文件循环使用 使用5个GC文件 每个GC文件的大小



负载均衡

**1、轮询法**

将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。

**2、随机法**

通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，

其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。

**3、源地址哈希法**

源地址哈希的思想是根据获取客户端的IP地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一IP地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。

**4、加权轮询法**

不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。给配置高、负载低的机器配置更高的权重，让其处理更多的请；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。

**5、加权随机法**

与加权轮询法一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重。不同的是，它是按照权重随机请求后端服务器，而非顺序。

**6、最小连接数法**

最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前

积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。



nginx

**1、轮询（默认）**

每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。  

**2、weight**

指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。

例如：  

```text
upstream bakend {  
  server 192.168.0.14 weight=10;  
  server 192.168.0.15 weight=10;  
}
```

**3、ip_hash**

每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。  

例如：  

```text
upstream bakend {  
  ip_hash;  
  server 192.168.0.14:88;  
  server 192.168.0.15:80;  
}
```

**4、fair（第三方）**

按后端服务器的响应时间来分配请求，响应时间短的优先分配。  

```text
upstream backend {  
  server server1;  
  server server2;  
  fair;  
}
```

**5、url_hash（第三方）** 

按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。  

例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。

```text
upstream backend {  
  server squid1:3128;  
  server squid2:3128;  
  hash $request_uri;  
  hash_method crc32;  
}
```



缓存系统一定程度上极大提升系统并发能力，但同样也增加额外技术考虑因素，比如缓存雪崩、缓存穿透、缓存更新与数据一致性

**4.1、缓存穿透**

缓存穿透是指查询的key不存在，从而缓存查询不到而查询了数据库。若是这样的key恰好并发请求很大，那么就会对数据库造成不必要的压力。比如黑客用一堆不存在的key访问数据，大量请求发送到数据库，数据库压力过大而宕机，怎么解决呢?

1、把所有存在的key都存到另外一个存储的Set集合里，查询时可以先查询key是否存在;

2、将这些key对应的值设置为null 丢到缓存里面去。后面再出现查询这个key 的请求的时候，直接返回null，再根据业务需求设置过期时间。

3、BloomFilter 类似于一个hbase set 用来判断某个元素（key）是否存在于某个集合中。这种方式在大数据场景应用比较多，比如 Hbase  中使用它去判断数据是否在磁盘上。这种方案可以加在第1/2种方案中，在缓存之前加一层 BloomFilter ，在查询的时候先去  BloomFilter 去查询 key 是否存在，如果不存在就直接返回，存在再走查缓存 -> 查 DB。

**4.2、缓存击穿**

在高并发的系统中，大量的请求同时查询一个 key  时，这个key刚好失效了，就会导致大量的请求都打到数据库上面去。这种现象我们称为缓存击穿。造成缓存击穿的原因是多个线程同时去查询数据库，那么我们可以在第一个查询数据的请求上使用一个 互斥锁。其他的线程进入等待状态，等第一个线程查询到了数据，然后做缓存。后面的线程进来发现已经有缓存了，就直接走缓存。

**4.3、缓存雪崩**

定义：缓存雪崩是指缓存系统失效，导致大量请求同时进行数据回源，导致数据源压力骤增而崩溃。两种情况会导致此问题：1、多个缓存数据同时失效；2、缓存系统崩溃，缓存同时失效

针对多个缓存数据同时失效的问题，可以将缓存时间离散化，根据缓存数据访问规律和缓存数据不一致的敏感性要求来选择缓存时间。

如果是第二个问题，缓存系统整体故障，则整个缓存系统不可用，大量回源请求，且由于缓存系统故障无法回写缓存，导致无法快速恢复。

这是缓存系统的引入，在解决高性能、高并发的同时，引入新的故障点。

考虑此问题，应从事前、事故中、事后不同阶段考虑：

事前：增加缓存系统高可用方案设计，避免出现系统性故障

事故中：增加多级缓存，在单一缓存故障时，仍有其他缓存系统可用，如之前项目中使用的三级缓存方案：内存级缓存->Memcached->Redis这样的方案；启用熔断限流机制，只允许可承受流量，避免全部流量压垮系统

事后：缓存数据持久化，在故障后快速恢复缓存系统





LRU

hashmap+double linked list





从名字我们就可以看到`ThreadLocal` 叫做本地线程变量，意思是说，`ThreadLocal` 中填充的的是当前线程的变量，该变量对其他线程而言是封闭且隔离的，`ThreadLocal` 为变量在每个线程中创建了一个副本，这样每个线程都可以访问自己内部的副本变量





GET 和 POST 只是 HTTP 协议中两种请求方式，所以在传输上，没有区别

报文格式上，不带参数时，最大区别仅仅是第一行方法名不同，一个是GET，一个是POST

带参数时报文的区别呢？在约定中，GET 方法的参数应该放在 url 中，POST 方法参数应该放在 body 中

按照网上大部分文章的解释，POST 比 GET 安全，因为数据在地址栏上不可见。
然而从传输的角度来说，他们都是不安全的，因为 HTTP 在网络上是明文传输，只要在网络节点上抓包，就能完整地获取数据报文。
要想安全传输，就只有加密，也就是 HTTPS

get能被缓存post不能

get能当书签，post不能

get参数会留在历史记录，post不会

get是在url加参数，url最长2048，post无限制

GET请求会被[浏览器缓存](https://so.csdn.net/so/search?q=浏览器缓存&spm=1001.2101.3001.7020)，所以对于相同的请求，浏览器会直接从缓存中获取数据，从而提高响应速度；而POST请求不会被缓存。



对象在堆里面，引用在栈里面

栈中的操作数**主要用于保存计算过程的中间结果，同时作为计算过程中变量临时的存储空间。**



自定义String 需要自定义加载器，继承ClassLoader 重写loadClass方法，打破双亲委派



虚拟内存作为缓存

内存管理 

内存保护

地址翻译



 64 位操作系统，因为进程最大可以申请 128 TB 大小的虚拟内存，即使物理内存只有 4GB，申请 8G 内存也是没问题，因为申请的内存是虚拟内存。如果这块虚拟内存被访问了，要看系统有没有 Swap 分区：

- 如果没有 Swap 分区，因为物理空间不够，进程会被OOM机制杀掉
- 如果有 Swap 分区，即使物理内存只有 4GB，程序也能正常使用 8GB 的内存，进程可以正常运行



Top k问题

1.堆：取文件中前K个数在内存中维护一个长度为K的小顶堆，然后从文件中挨个读取数字并和堆顶比较，如果比堆顶小则直接丢弃，否则替换堆顶后调整小顶堆。遍历完文件中所有的数字后，小顶堆中的K个数就是所求的Top K。

2.快排

- step1：以数组最后一个元素作为基准值，将数据partition为`[left,mid-1]`和`[mid,right]`两个区间，其中`[left,mid-1]`的数都小于base值，`[mid,right]`都大于等于base值

- step2：计算`[mid, right]`数组长度L：

- - 如果L等于K那么`[mid,right]`这些数字就是最终要找到的Tok K个数
  - 如果L大于K则对`[mid, right]`重复step1操作
  - 如果长度小于K说明`[mid, right]`中都是Top K的数，我们将问题转化为在`[left, mid-1]`中寻找Top K - L个数（如果L较小时也可以对partition前的数据`[left, right]`直接进行排序，然后取最大的K个数即可）

3.局部淘汰

最极端时K为1，那么我们可以自己实现max函数找到序列最小值，时间复杂度为  ，空间复杂度为 

 。当K大于等于2时，我们需要维护一个长度为K的序列来保存最大的K个数，具体思路如下：

- step1：使用一个数组存储文件前100个浮点数，并排好序，记为序列L
- step2：遍历文件中剩余的数字，如果比序列L中最小值还要小则直接丢弃，否则通过插入排序的方式插入序列L并删掉最小数，最终得到的序列L就是前100大的数



索引命中

1、如果条件中有 or ，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因）

注意：要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
如果出现OR的一个条件没有索引时，建议使用 union ，拼接多个查询语句
2.、like查询是以%开头，索引不会命中
**只有一种情况下，只查询索引列，才会用到索引，但是这种情况下跟是否使用%没有关系的，因为查询索引列的时候本身就用到了索引**
\3. 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引
\4. 没有查询条件，或者查询条件没有建立索引
\5. 查询条件中，在索引列上使用函数（+, - ,*,/）, 这种情况下需建立函数索引
\6. 采用 not in, not exist
\7. B-tree 索引 is null 不会走， is not null 会走





并发三个特性

原子性，

即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。

可见性，

指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。

有序性

即程序执行的顺序按照代码的先后顺序执行。



保证可见性，不保证原子性

当写一个volatile变量时，JMM会把该线程本地内存中的变量强制刷新到主内存中去；

禁止指令重排



缓存与数据库不一致

但是一旦涉及到数据更新：数据库和缓存更新，就容易出现缓存(Redis)和数据库（MySQL）间的数据一致性问题。

**方法一：采用延时双删策略**

（1）先删除缓存（2）再写数据库（3）休眠500毫秒（4）再次删除缓存

为什么要双删呢？如果你们的删缓存失败了，怎么办？那不是还是会出现缓存和数据库不一致的情况么？比如一个写数据请求，然后写入数据库了，删缓存失败了，这会就出现不一致的情况了。

**方法二：异步更新缓存(基于订阅binlog的同步机制)****技术整体思路：**MySQL binlog增量订阅消费+消息队列+增量数据更新到redis1）读Redis：热数据基本都在Redis2）写MySQL:增删改都是操作MySQL3）更新Redis数据：MySQ的数据操作binlog，来更新到Redis

2.2.2、Redis更新过程

数据操作主要分为两种：

1、一种是全量（将所有数据一次性写入Redis）
 2、一种是增量（实时更新）





1.ConcurrentHashMap维护了一个Node数组（JDK1.8），保存了各节点链表的头节点。

2，当链表长度超过8时，ConcurrentHashMap会考虑把链表转为红黑树，但不一定真的转。

3，当链表长度超过8，但Node数组长度小于64时，优先考虑数组扩容。如果Node数组长度大于64，则把链表转为红黑树。



.如果两个对象相同，那么它们的hashCode值一定要相同
　
2.如果两个对象的hashCode相同，它们并不一定相同（这里说的对象相同指的是用eqauls方法比较）。
如不按要求去做了，会发现相同的对象可以出现在Set集合中，同时，增加新元素的效率会大大下降。
3.equals()相等的两个对象，hashcode()一定相等；equals()不相等的两个对象，却并不能证明他们的hashcode()不相等。
换句话说，equals()方法不相等的两个对象，hashcode()有可能相等（我的理解是由于哈希码在生成的时候产生冲突造成的）。反过来，hashcode()不等，一定能推出equals()也不等；hashcode()相等，equals()可能相等，也可能不等。

自反性（reflexive）。对于任意不为null的引用值x，x.equals(x)一定是true。

对称性（symmetric）。对于任意不为null的引用值x和y，当且仅当x.equals(y)是true时，y.equals(x)也是true。

传递性（transitive）。对于任意不为null的引用值x、y和z，如果x.equals(y)是true，同时y.equals(z)是true，那么x.equals(z)一定是true。

一致性（consistent）。对于任意不为null的引用值x和y，如果用于equals比较的对象信息没有被修改的话，多次调用时x.equals(y)要么一致地返回true要么一致地返回false。



https不是绝对安全

网络嗅探 渗透



concurrentHashMap 高频

- hashmap：
- capacity：当前数组容量，始终保持 2^n，可以扩容，扩容后数组大小为当前的 2 倍。
- loadFactor：负载因子，默认为 0.75。
- threshold：扩容的阈值，等于 capacity * loadFactor

segment数组，不可扩容；segment中的内部数组和链表，内部数组可扩容

concurrencyLevel： 并行级别、并发数、Segment 数，默认值是16，也就是说 ConcurrentHashMap 有 16 个 Segments，所以理论上，这个时候，最多可以同时支持 16 个线程并发写，只要它们的操作分别分布在不同的 Segment 上。这个值可以在初始化的时候设置为其他值，但是一旦初始化以后，它是不可以扩容的
JDK1.8的实现降低锁的粒度，JDK1.7版本锁的粒度是基于Segment的，包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry（首节点）

CAS

- synchronized是悲观锁，这种线程一旦得到锁，其他需要锁的线程就挂起的情况就是悲观锁。
- CAS操作的就是乐观锁，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。

所谓原子操作类，指的是java.util.concurrent.atomic包下，一系列以Atomic开头的包装类。例如`AtomicBoolean`，`AtomicInteger`，`AtomicLong`。它们分别用于`Boolean`，`Integer`，`Long`类型的原子性操作。

CAS机制当中使用了3个基本操作数：内存地址V，旧的预期值A，要修改的新值B。

更新一个变量的时候，只有当变量的预期值A和内存地址V当中的实际值相同时，才会将内存地址V对应的值修改为B

1.CPU开销较大
 在并发量比较高的情况下，如果许多线程反复尝试更新某一个变量，却又一直更新不成功，循环往复，会给CPU带来很大的压力。

2.不能保证代码块的原子性
 CAS机制所保证的只是一个变量的原子性操作，而不能保证整个代码块的原子性。比如需要保证3个变量共同进行原子性的更新，就不得不使用Synchronized了。



LRU linkedHashMap

io那些

jdk cglib 

http1.0 1.1 2.0  https://www.jianshu.com/p/cd70b8e90d00

1.长连接

在HTTP/1.0中，默认使用的是短连接，也就是说每次请求都要重新建立一次连接。 HTTP 是基于TCP/IP协议的,每一次建立或者断开连接都需要三次握手四次挥手的开销，如果每次请求都要这样的话，开销会比较大。因此最好能维持一个长连接，可以用长连接来发多个请求。HTTP 1.1起，默认使用长连接 ,默认开启Connection： keep-alive。 HTTP/1.1的持续连接有非流水线方式和流水线方式 。流水线方式是客户在收到HTTP的响应报文之前就能接着发送新的请求报文。与之相对应的非流水线方式是客户在收到前一个响应后才能发送下一个请求。
2.错误状态响应码

在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。
3.缓存处理

在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。
4.带宽优化及网络连接的使用

HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。




为什么在重写 equals 方法的时候需要重写 hashcode 方法

Set 集合是用来保存不同对象的，相同的对象就会被 Set 合并，最终留下一份独一无二的数据。

即使两个对象是相等的，Set 集合竟然没有将二者进行去重与合并。这就是重写了 equals 方法，但没有重写 hashCode 方法的问题所在。

那么默认情况下，Set 进行去重操作时，会先判断两个对象的 hashCode 是否相同，此时因为没有重写 hashCode 方法，所以会直接执行 Object 中的 hashCode 方法，而 Object 中的 hashCode 方法对比的是两个不同引用地址的对象，所以结果是  false，那么 equals 方法就不用执行了，直接返回的结果就是 false：两个对象不是相等的，于是就在 Set  集合中插入了两个相同的对象。





synchoronized reentrantlock

**底层实现**上来说，synchronized 是**JVM**层面的锁，是**Java关键字**，通过monitor对象来完成（monitorenter与monitorexit），对象只有在同步块或同步方法中才能调用wait/notify方法，ReentrantLock 是从jdk1.5以来（java.util.concurrent.locks.Lock）提供的**API层面**的锁。

synchronized 的实现涉及到锁的升级，具体为无锁、偏向锁、自旋锁、向OS申请重量级锁，ReentrantLock实现则是通过利用CAS（CompareAndSwap）自旋机制保证线程操作的原子性和volatile保证数据可见性以实现锁的功能。

 **是否可手动释放：**

synchronized 不需要用户去手动释放锁，synchronized 代码执行完后系统会自动让线程释放对锁的占用；   ReentrantLock则需要用户去手动释放锁，如果没有手动释放锁，就可能导致死锁现象。一般通过lock()和unlock()方法配合try/finally语句块来完成，使用释放更加灵活。

**是否可中断**

synchronized是不可中断类型的锁，除非加锁的代码中出现异常或正常执行完成；  ReentrantLock则可以中断，可通过trylock(long timeout,TimeUnit  unit)设置超时方法或者将lockInterruptibly()放到代码块中，调用interrupt方法进行中断。

**是否公平锁**

synchronized为非公平锁  ReentrantLock则即可以选公平锁也可以选非公平锁，通过构造方法new ReentrantLock时传入boolean值进行选择，为空默认false非公平锁，true为公平锁。

**锁是否可绑定条件Condition**

synchronized不能绑定；   ReentrantLock通过绑定Condition结合await()/singal()方法实现线程的精确唤醒，而不是像synchronized通过Object类的wait()/notify()/notifyAll()方法要么随机唤醒一个线程要么唤醒全部线程。

幂等

所谓的幂等性，是分布式环境下的一个常见问题，一般是指我们在进行多次操作时，所得到的结果是一样的，即多次运算结果是一致的。

为什么链表变红黑树 hashmap

因为红黑树需要进行左旋，右旋操作， 而单链表不需要，
 如果元素小于8个，查询成本高，新增成本低
 如果元素大于8个，查询成本低，新增成本高 



在Java 8中，链表转换为[红黑树](https://so.csdn.net/so/search?q=红黑树&spm=1001.2101.3001.7020)的阈值为64。这个值是由Java开发人员根据性能测试结果确定的，旨在在保证性能的同时尽可能减少红黑树的使用。



hashmap为什么2的次幂

*为了数据的的均匀分布,减少hash冲突*

string stringbuilder stringbuffer

| String                                                       | StringBuffer                                                 | StringBuilder                                                |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| String的值是不可变的，这就导致每次对String的操作都会生成新的String对象，不仅效率低下，而且浪费大量优先的内存空间 | StringBuffer是可变类，和线程安全的字符串操作类，任何对它指向的字符串的操作都不会产生新的对象。每个StringBuffer对象都有一定的缓冲区容量，当字符串大小没有超过容量时，不会分配新的容量，当字符串大小超过容量时，会自动增加容量 | 可变类，速度更快                                             |
| 不可变                                                       | 可变                                                         | 可变                                                         |
|                                                              | 线程安全                                                     | 线程不安全                                                   |
|                                                              | StringBuffer是单线程的，因为里面的方法都是被synchronized修饰了。所以它[线程安全](https://so.csdn.net/so/search?q=线程安全&spm=1001.2101.3001.7020)，但是效率自然就降低。 **多线程操作字符串建议使用StringBuffer。** | StringBuilder不是单线程的，因为里面的方法没有被synchronized修饰了。所以它线程不安全，所以效率要更高。 **单线程操作字符串建议使用StringBuilder** |

StringBuilder在进行append连接字符串的时候并不是用String存储，而是存放到一个名为value的char数组当中，字符串是固定长度的，而数组是可以扩容的，这样就不需要不停创建对象了。
 默认16

扩容系数是value.length * 2 + 2，

cookie session

Cookie通过在客户端记录信息确定用户身份，Session通过在服务器端记录信息确定用户身份

1、cookie数据存放在客户端，session数据放在服务器上。

2、cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗，考虑到安全应当使用session。

3、session会在一定时间内保存在服务器上，当访问增多，会比较占用你服务器的性能，考虑性能应当使用cookie。

4、不同浏览器对cookie的数据大小限制不同，个数限制也不相同。单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。

5、可以考虑将登陆信息等重要信息存放为session，不重要的信息可以放在cookie中。



list和set

锁机制

不用redis怎么缓存

怎么保证分布式事务

高并发下如何统计在线人数

1）表统计：用数据表统计在线人数，缺点是当并发量大的时候可能造成性能瓶颈，如无特别大的并发的时候完全可以胜任

2）Redis有序集合：因为在内存中，所以效率很高，可以统计某个时间段内的在线人数，还可以做各种聚合操作。但是如果在线人数比较多的情况下，会比较占用内存。

3）Redis
HyperLogLog：占用空间小，但是这个是个概率算法，只能给出估算值，并且无法统计一段时间内的人数，也无法知道是哪些用户在线，非常适合数据量特别大的情况。

4）Redis bitmap：推荐方式，占用空间小，能够详细记录哪些用户在线。

泛型，泛型擦除

多态用在那里

1.消除类型之间的耦合关系

\2. 可替换性

\3. 可扩充性

\4. 接口性

\5. 灵活性

\6. 简化性

可以用在方法的参数中和方法的返回类型中

类型的参数.因为Object是Java中所有类的基类.,但是才传入参数的时候,可以传入任何一个类的对象
**这就是多态的应用!**

垃圾回收算法 g1



Exception

dump

select* 和select字段的区别

查询效率上：select * 在系统解析的时候会多一步从系统表获取具体字段的步骤，因此会比select 全部字段多花时间，效率稍低。 

查询结果上：在表结构不修改的情况下结果相同，但是后者的顺序可以调整，前者则固定；而如果修改了表结构，前者能够获得新表结构的所有字段，后者则会在修改字段名或删除字段时报错，会在增加字段时不会输出新字段。 

应用场景上：select * 效率稍低但是能应对频繁调整的表结构，适应力强，可应用于开发环境，仅极少数特殊业务场景会在生产环境使用场景；后者效率稍高语意明确，更能清晰的表达业务的需求，强烈建议在生产环境中使用。 



设计模式

spring和boot



如何解决TCP粘包

因为TCP是面向流，没有边界，而操作系统在发送TCP数据时，会通过缓冲区来进行优化，例如缓冲区为1024个字节大小。

- 发送端将每个包都封装成固定的长度，比如100字节大小。如果不足100字节可通过补0或空等进行填充到指定长度；
- 发送端在每个包的末尾使用固定的分隔符，例如\r\n。如果发生拆包需等待多个包发送过来之后再找到其中的\r\n进行合并；例如，FTP协议；
- 将消息分为头部和消息体，头部中保存整个消息的长度，只有读取到足够长度的消息之后才算是读到了一个完整的消息；
- 通过自定义协议进行粘包和拆包的处理。



如何设置索引



threadlocal底层原理，key和value分别是什么

ThreadLocal 方法提供泛型的版本, 即该变量中存放的类型可以自己指定。声明该变量的时候，直接通过 new 方法就可以了。

ThreadLocalMap 是 ThreadLocal 类中的内部静态类。每个线程都有一个 ThreadLocalMap  类变量，该类存放了键值对，其中键值对的 key 就是 ThreadLocal 的一个引用，value 就是 ThreadLocal  对应的值。ThreadLocal 实例本身并没有存储值，值都是存放在 ThreadLocalMap 中，ThreadLocal  的作用就是作为键值对中的一个 key。

ThreadLocalMap 使用 ThreadLocal 的弱引用作为 key，弱引用 ThreadLocal 被回收后，导致  ThreadLocalMap 中出现 key 为 null 的数据，这些数据的 value 值就会一直存在着引用链，导致无法 GC  回收，造成内存泄漏。所以使用完 ThreadLocal 后，最好调用 remove() 方法，清除数据。



String类可以被继承吗

双亲委托机制如何打破

## 打破双亲委派

打破双亲委派的两种方式：

1.通过spi机制，使用ServiceLoader.load去加载

2.通过自定义类加载器，继承classloader，重写loadclass方法

### SPI机制

spi机制是一种服务发现机制。它通过在ClassPath路径下的META-INF/services文件夹查找文件，自动加载文件里所定义的类。这一机制为很多框架扩展提供了可能，比如在JDBC中就使用到了SPI机制。



url如何转为ip 域名对应的ip不止一个 负载均衡



模式查询命中索引的条件



MVCC

Spring bean的生存周期和作用域

BeanFactory factoryBean

spring aop的底层实现



超卖超买

B B+ 红黑树

redis分布式锁

非重入锁

- 基于 wait/notify 实现不可重入锁
- 自旋锁，即获取锁的线程在锁被占用时，不是阻塞，而是不断循环去尝试，直到获取锁。
- 可重入锁 synchronized
- 可重入锁 ReentrantLock 也可以设置true和false变为不可重入

sql优化

- 最大化利用索引；
- 尽可能避免全表扫描；
- 减少无效数据的查询；



线程具有许多传统进程所具有的特征，故又称为轻型进程(Light—Weight Process)或进程元；而把传统的进程称为重型进程(Heavy—Weight Process)，它相当于只有一个线程的任务。在引入了线程的操作系统中，通常一个进程都有若干个线程，至少包含一个线程。

根本区别：进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位

资源开销：每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小。

包含关系：如果一个进程内有多个线程，则执行过程不是一条线的，而是多条线（线程）共同完成的；线程是进程的一部分，所以线程也被称为轻权进程或者轻量级进程。

内存分配：同一进程的线程共享本进程的地址空间和资源，而进程之间的地址空间和资源是相互独立的

影响关系：一个进程崩溃后，在保护模式下不会对其他进程产生影响，但是一个线程崩溃整个进程都死掉。所以多进程要比多线程健壮。

执行过程：每个独立的进程有程序运行的入口、顺序执行序列和程序出口。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制，两者均可并发执行


问题

**脏读**

脏读指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。读到了并一定最终存在的数据，这就是脏读。

**可重复读**

可重复读指的是在一个事务内，最开始读到的数据和事务结束前的任意时刻读到的同一批数据都是一致的。通常针对数据**更新（UPDATE）**操作。

**不可重复读**

对比可重复读，不可重复读指的是在同一事务内，不同的时刻读到的同一批数据可能是不一样的，可能会受到其他事务的影响，比如其他事务改了这批数据并提交了。通常针对数据**更新（UPDATE）**操作。

**幻读**

幻读是针对数据**插入（INSERT）**操作来说的。假设事务A对某些行的内容作了更改，但是还未提交，此时事务B插入了与事务A更改前的记录相同的记录行，并且在事务A提交之前先提交了，而这时，在事务A中查询，会发现好像刚刚的更改对于某些数据未起作用，但其实是事务B刚插入进来的，让用户感觉很魔幻，感觉出现了幻觉，这就叫幻读。

四种隔离

1. 读未提交（READ UNCOMMITTED）
2. 读提交 （READ COMMITTED）
3. 可重复读 （REPEATABLE READ）
4. 串行化 （SERIALIZABLE）

**读未提交**

MySQL 事务隔离其实是依靠锁来实现的，加锁自然会带来性能的损失。而**读未提交隔离级别是不加锁**的，所以它的性能是最好的，没有加锁、解锁带来的性能开销。但有利就有弊，这基本上就相当于裸奔啊，所以它连脏读的问题都没办法解决。

任何事务对数据的修改都会第一时间暴露给其他事务，即使事务还没有提交

**读提交**

既然读未提交没办法解决脏数据问题，那么就有了读提交。读提交就是一个事务只能读到其他事务已经提交过的数据，也就是其他事务调用 commit 命令之后的数据。那脏数据问题迎刃而解了。

**可重复读**

可重复是对比不可重复而言的，上面说不可重复读是指同一事物不同时刻读到的数据值可能不一致。而可重复读是指，事务不会读到其他事务对已有数据的修改，及时其他事务已提交，也就是说，事务开始时读到的已有数据是什么，在事务提交前的任意时刻，这些数据的值都是一样的。但是，对于其他事务新插入的数据是可以读到的，这也就引发了幻读问题。

**串行化**

串行化是4种事务隔离级别中隔离效果最好的，解决了脏读、可重复读、幻读的问题，但是效果最差，它将事务的执行变为顺序执行，与其他三个隔离级别相比，它就相当于单线程，后一个事务的执行必须等待前一个事务结束。